## SNAPSHOT ENSEMBLES: TRAIN 1, GETM FOR FREE 笔记

### 主要思想
这篇论文的主要思想依赖于几个发现

- 每个模型的局部最优解很多，随着模型参数的增加指数增长
- 在训练是，降低学习率会使得模型迅速收敛，即使学习率降低的比较早，也会收敛，只是效果较差
- 周期学习率的训练方法的提出
- 局部最优解分错的样本没有比较大的overlap, 因此可以通过集成的方法互相弥补，增强泛化能力

基于以上几个发现，论文提出了在一次模型训练过程中，使用m个周期的CLR方法让模型收敛于不同的局部最优解得到M个不同参数但是相同结构的模型，进行集成。测试时选出后t个模型进行预测结果的平均。

思想图解

![](images/snapshot-ensemble-1.jpg?raw=true "title")

图中左边是一个SGD训练过程，最终收敛于一个平缓的局部最优解，右边是快照集成的训练方法，在训练过程中进入3个不同的局部最优解。

### 训练方式

![](images/snapshot-ensemble-2.jpg?raw=true "title")

从图像中可以看出训练了6个周期，每个周期结束后，学习率突增，每个周期内学习率随着迭代次数进行更新。

更新公式为：
![](images/snapshot-ensemble-3.jpg?raw=true "title")
其中α0是初始学习率，t是迭代次数，T是总得迭代次数，M是周期个数